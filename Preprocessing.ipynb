{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4448b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from langchain.schema.document import Document\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.document_loaders import DirectoryLoader, UnstructuredPDFLoader\n",
    "from langchain.document_loaders.text import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d2e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads a folder as data_path\n",
    "DATA_PATH = \"rag_data/oecd_policy_outlook\"\n",
    "#loads datapath for structured pdf documents\n",
    "def load_documents():\n",
    "    pdfdocument_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return pdfdocument_loader.load()\n",
    "\n",
    "#loads datapath for unstructured pdf documents, performs ocr on files if necessary\n",
    "def load_unstructered_documents():\n",
    "    def unstructured_pdf_loader(file_path):\n",
    "        return UnstructuredPDFLoader(\n",
    "            file_path,\n",
    "            mode=\"elements\",\n",
    "            strategy=\"auto\",\n",
    "        )\n",
    "    # DirectoryLoader with glob for only PDFs and custom loader\n",
    "    unstructured_loader = DirectoryLoader(\n",
    "        path=DATA_PATH,\n",
    "        glob=\"**/*.pdf\",  # recursively find all PDFs\n",
    "        loader_cls=unstructured_pdf_loader\n",
    "    )\n",
    "    return unstructured_loader.load()\n",
    "\n",
    "#loads datapath for txt files\n",
    "def load_txt_files():\n",
    "    def custom_text_loader(DATA_PATH):\n",
    "        return TextLoader(DATA_PATH, encoding=\"utf-8\")\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"**/*.txt\", loader_cls=custom_text_loader)\n",
    "    return loader.load()\n",
    "\n",
    "documents = load_documents() #loads pdf documents\n",
    "#documents = load_unstructered_documents() #loads unstructured pdf documents, needed for iee, wee and worldbank\n",
    "#documents = load_txt_files() #loads txt files needed for wetten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c6fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set chunksize and overlap\n",
    "CHUNK_SIZE = 512   #character-based chunk size\n",
    "OVERLAP = 40       #character overlap\n",
    "\n",
    "#Initialises the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],  #Splits on sentences and or paragraphs first\n",
    ")\n",
    "\n",
    "#creates chunks out of documents using the chunk_size and overlap\n",
    "def chunk_documents(documents):\n",
    "    chunks = []\n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        full_text = doc.page_content\n",
    "        split_texts = text_splitter.split_text(full_text)\n",
    "        offset = 0\n",
    "\n",
    "        #finds chunk position to return in meta data\n",
    "        for i, chunk in enumerate(split_texts):\n",
    "            start = full_text.find(chunk, offset)\n",
    "            if start == -1:\n",
    "                #fallback\n",
    "                start = offset\n",
    "            end = start + len(chunk)\n",
    "            offset = end  #moves offset forward\n",
    "\n",
    "            chunks.append({\n",
    "                \"content\": chunk,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"doc_idx\": doc_idx,\n",
    "                \"chunk_index\": i\n",
    "            })\n",
    "    return chunks\n",
    "\n",
    "#uses chunk_documents to chunk a document\n",
    "chunks = chunk_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec759b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialises bge-m3\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={\"device\": \"cuda\"},  # or \"cpu\" if no GPU\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09782459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeds chunks from documents, adds metadata to the chunks\n",
    "def embed_chunks(original_documents, chunk_data):\n",
    "    embedded_documents = []\n",
    "\n",
    "    for chunk_info in chunk_data:\n",
    "        chunk = chunk_info[\"content\"]\n",
    "        doc_idx = chunk_info[\"doc_idx\"]\n",
    "        i = chunk_info[\"chunk_index\"]\n",
    "        chunk_start = chunk_info[\"start\"]\n",
    "        chunk_end = chunk_info[\"end\"]\n",
    "\n",
    "        source_doc = original_documents[doc_idx]\n",
    "        doc_meta = source_doc.metadata if hasattr(source_doc, \"metadata\") else {}\n",
    "        \n",
    "        #retrieves metadata\n",
    "        metadata = {\n",
    "            \"source\": doc_meta.get(\"source\", f\"doc_{doc_idx}\"),\n",
    "            \"document_id\": doc_meta.get(\"document_id\", f\"doc_{doc_idx}\"),\n",
    "            \"title\": doc_meta.get(\"title\"),\n",
    "            \"author\": doc_meta.get(\"author\"),\n",
    "            \"created_at\": doc_meta.get(\"created_at\"),\n",
    "            \"chunk_index\": i,\n",
    "            \"chunk_char_start\": chunk_start,\n",
    "            \"chunk_char_end\": chunk_end,\n",
    "            \"language\": doc_meta.get(\"language\", \"en\"),\n",
    "        }\n",
    "\n",
    "        #adds the metadata to the chunks\n",
    "        embedded_documents.append(Document(\n",
    "            page_content=chunk,\n",
    "            metadata=metadata\n",
    "        ))\n",
    "\n",
    "    #embeds all chunks\n",
    "    embeddings = embedding_model.embed_documents(\n",
    "        [doc.page_content for doc in embedded_documents]\n",
    "    )\n",
    "\n",
    "    return embedded_documents, embeddings\n",
    "\n",
    "embedded_docs, embeddings = embed_chunks(documents, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca53c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adds documents to faiss vector database in index_dir. If there is not index_dir creates index_dir\n",
    "INDEX_DIR = \"faiss_index_bge_m3\"\n",
    "\n",
    "if os.path.exists(INDEX_DIR):\n",
    "    #adds new chunks to vector db, by adding embeddings\n",
    "    index = FAISS.load_local(INDEX_DIR, embeddings=embedding_model, allow_dangerous_deserialization=True)\n",
    "    index.add_documents(embedded_docs)\n",
    "    index.save_local(INDEX_DIR)\n",
    "else:\n",
    "    #creates an empty index and adds documents to this index\n",
    "    index = FAISS.from_documents(embedded_docs, embedding=embedding_model)\n",
    "    index.save_local(INDEX_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adds documents to faiss vector database in index_dir. If there is not index_dir creates index_dir\n",
    "#here it is done in batching to make it possible with less computing power (RAM)\n",
    "BATCH_SIZE = 400  # adjust based on your RAM capacity\n",
    "\n",
    "num_batches = math.ceil(len(embedded_docs) / BATCH_SIZE)\n",
    "\n",
    "if os.path.exists(INDEX_DIR):\n",
    "    #adds new chunks to vector db, by adding embeddings\n",
    "    index = FAISS.load_local(INDEX_DIR, embeddings=embedding_model, allow_dangerous_deserialization=True)\n",
    "    for i in range(num_batches):\n",
    "        batch_docs = embedded_docs[i * BATCH_SIZE : (i + 1) * BATCH_SIZE]\n",
    "    index.add_documents(batch_docs)\n",
    "    index.save_local(INDEX_DIR)\n",
    "else:\n",
    "    #creates an empty index and adds documents to this index\n",
    "    for i in range(num_batches):\n",
    "        batch_docs = embedded_docs[i * BATCH_SIZE : (i + 1) * BATCH_SIZE]\n",
    "    index = FAISS.from_documents(batch_docs, embedding=embedding_model)\n",
    "    index.save_local(INDEX_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
