{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9573b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter your openai api key, if this is unavailable another model should be loaded instead of gpt-4o\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c22bb7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raf\\AppData\\Local\\Temp\\ipykernel_6352\\417847963.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "#load the embedding model on gpu\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={\"device\": \"cuda\"},  # or \"cpu\" if no GPU\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa506562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS index and retriever, embedding model should be the same as for the created embeddings in the vector database\n",
    "faiss_index = FAISS.load_local(\"faiss_index_bge_m3\", embedding_model, allow_dangerous_deserialization=True)\n",
    "retriever = faiss_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1000})  #loads the 1000 most similar to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2804190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reranker model (it can be loaded for gpu but not in current code)\n",
    "reranker_model_name = \"BAAI/bge-reranker-v2-m3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name)\n",
    "\n",
    "def rerank(query: str, docs: list, top_k: int = 10):\n",
    "    pairs = [(query, doc.page_content) for doc in docs]  #Assumes input to be langchain documents\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits.view(-1)\n",
    "    \n",
    "    scores = F.softmax(logits, dim=0)\n",
    "    scored_docs = sorted(zip(docs, scores.tolist()), key=lambda x: x[1], reverse=True)\n",
    "    top_docs = [doc for doc, score in scored_docs[:top_k]]\n",
    "    return top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03626b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raf\\AppData\\Local\\Temp\\ipykernel_6352\\2842050412.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "# Initialize gpt-4o\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=16384  # the maximum of output tokens gpt-4o can generate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "747a9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt template\n",
    "system_template = \"\"\"\n",
    "Your task is: {task}\n",
    "You are acting as a {persona}. \n",
    "\n",
    "Use the following context to inform your answer, but keep in mind all Dutch-language texts are about Dutch educational policies. English-language texts are about international contexts:\n",
    "-------------------\n",
    "{context}\n",
    "-------------------\n",
    "\n",
    "Here are examples of how answers should be structured:\n",
    "{exemplars}\n",
    "\n",
    "Respond in a {tone} tone, and format the answer as follows:\n",
    "{format}\n",
    "\n",
    "Do not skip any of the documents given. \n",
    "Always give a result. Do not repeat the same results.\n",
    "\"\"\"\n",
    "#input variables can be filled in seperately\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"persona\", \"task\", \"context\", \"exemplars\", \"tone\", \"format\"],\n",
    "    template=system_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc1f5099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your actual query/question for retrieval\n",
    "query=\"Overview of the key laws, legal frameworks, regulations, rules and recent policy changes affecting education in the Netherlands.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fbaa39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raf\\AppData\\Local\\Temp\\ipykernel_6352\\2778122325.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "#retrieve documents based on query\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "#rerank documents based on query and documents returning the 100 best documents and add metadata\n",
    "#higher top-k is possible, but should still fit in the model context window\n",
    "reranked_docs = rerank(query, retrieved_docs, top_k=100)\n",
    "relevant_docs = \"\\n\\n\".join([\n",
    "    f\"Document ID: {doc.metadata.get('document_id', 'N/A')} | \"\n",
    "    f\"Title: {doc.metadata.get('title', 'N/A')} | \"\n",
    "    f\"Source: {doc.metadata.get('source', 'N/A')}\\n\"\n",
    "    f\"Chunk: {doc.metadata.get('chunk_index', 'N/A')} | \"\n",
    "    f\"Chars: [{doc.metadata.get('chunk_char_start', 'N/A')}â€“{doc.metadata.get('chunk_char_end', 'N/A')}]\\n\"\n",
    "    f\"Content:\\n{doc.page_content.strip()}\"\n",
    "    for doc in reranked_docs\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c42b4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fills inputs in prompt template\n",
    "#for every query this needs to be reran, to insert the correct retrieved documents or context\n",
    "inputs_csv = {\n",
    "    \"persona\": \"senior educational policy analyst. Your task is to extract educational policy changes {subtask} from the given context\",\n",
    "    \"task\": \"Extract as many educational policy changes {subtask} from the retrieved content.\",\n",
    "    \"context\": relevant_docs,  # Injected retrieved documents here\n",
    "    \"exemplars\": \"Create a table which exists of the country, year, title of the educational policy change and a description of this change. If the year is unknown leave it blank. Do not limit the number of columns in the output\",\n",
    "    \"tone\": \"Use a neutral, historical, and informative tone to describe educational developments.\",\n",
    "    \"format\": \"A table or Csv file including rows of country, year, educational change and a brief description of this educational change. Do not limit the number of columns in the output, include all available entries if possible.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ab2e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collection of subtasks given to the model which lead combined with a query on this topic lead to answers in a part of educational policy\n",
    "subtask = \"in the Netherlands\"\n",
    "subtask_primary = \"on primary education in the Netherlands\"\n",
    "subtask_secondary = \"on secondary education, like the vmbo, hbo and vwo in the Netherlands\"\n",
    "subtask_higher = \"on higher education, like hogescholen and univerities in the Netherlands\"\n",
    "subtask_vocational = \"on vocational education like the mbo and adult education in the Netherlands\"\n",
    "subtask_system = \"on the structure of education, curriculum and the role of regulatory bodies in the Netherlands\"\n",
    "subtask_programs = \"on educational programs in the netherlands\"\n",
    "subtask_teacher = \"on teachers, teacher programs, teachers education and their working conditions in the Netherlands\"\n",
    "subtask_loans = \"on student loans and student grants in the Netherlands\"\n",
    "subtask_disadvantage = \"on programs for disadvantaged students and special education in the Netherlands\"\n",
    "subtask_religion = \"on religious education in the Netherlands\"\n",
    "subtask_exams = \"on the exams and language and arithmetic tests in education in the Netherlands.\"\n",
    "subtask_finance = \"on school finances, including salaries and budgets in the Netherlands\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5b8330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fills the input of the subquery with the subtask, change both subtasks to the searched for part of educational policy\n",
    "inputs_csv[\"persona\"] = inputs_csv[\"persona\"].format(subtask=subtask)\n",
    "inputs_csv[\"task\"] = inputs_csv[\"task\"].format(subtask=subtask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b8a9217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raf\\AppData\\Local\\Temp\\ipykernel_6352\\1104876651.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  rag_chain = LLMChain(llm=llm, prompt=prompt)\n",
      "C:\\Users\\raf\\AppData\\Local\\Temp\\ipykernel_6352\\1104876651.py:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = rag_chain.run(inputs_csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a table summarizing the educational policy changes in the Netherlands based on the provided context:\n",
      "\n",
      "| Country     | Year | Title of Educational Policy Change | Description of Educational Change                                                                                                                                                                                                 |\n",
      "|-------------|------|------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| Netherlands | 2010 | Language and Numeracy Act          | The Ministry of Education, Culture and Science developed additional reference levels for literacy and numeracy, which were incorporated into the education system to enhance learning outcomes in these fundamental areas.          |\n",
      "| Netherlands | 2013 | New Dutch Laws on Student Assessment | New laws on student assessment were developed, focusing on improving the evaluation mechanisms within the education system to better monitor and enhance student performance.                                                        |\n",
      "| Netherlands | 2016 | OECD Education Policy Review       | The OECD conducted a review of the Dutch education system to provide policymakers, educators, and stakeholders with an external analysis aimed at improving the quality and outcomes of the education system.                        |\n",
      "\n",
      "This table captures the key educational policy changes and initiatives in the Netherlands as extracted from the provided documents.\n"
     ]
    }
   ],
   "source": [
    "#Run the filled out prompt in the llm given (gpt-4o)\n",
    "rag_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "answer = rag_chain.run(inputs_csv)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
