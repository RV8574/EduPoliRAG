{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8836bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09561fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name url to scrape from, output directory and the number of pages to scrape\n",
    "start_url = \"https://zoek.officielebekendmakingen.nl/resultaten?q=(c.product-area==%22officielepublicaties%22)and((dt.subject=%22basisonderwijs%22)or(dt.subject=%22beroepsonderwijs%22)or(dt.subject=%22kenniseconomie%22)or(dt.subject=%22Onderwijs%20en%20wetenschap%22)or(dt.subject=%22onderwijsvoorzieningen%22))and((w.publicatienaam==%22Staatsblad%22))&zv=&pg=50&col=Staatsblad&svel=Publicatiedatum&svol=Aflopend\"\n",
    "output_dir = \"officielebekendmakingen\"\n",
    "max_pages = 31  #Total number of pages to scrape\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#Setup Selenium\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "#code loops over pages downloading all pdfs it can find\n",
    "for page in range(1, max_pages + 1):\n",
    "    print(f\"Scraping page {page}...\") #tells which page you are on\n",
    "\n",
    "    if page == 1:\n",
    "        url = start_url\n",
    "    else:\n",
    "        url = f\"{start_url}&pagina={page}\"\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    #Finds all PDF links on the current page\n",
    "    links = driver.find_elements(By.XPATH, \"//a[contains(@href, '.pdf')]\")\n",
    "    print(f\"Found {len(links)} PDFs\") #prints number of pdfs found\n",
    "\n",
    "    for link in links:\n",
    "        pdf_url = link.get_attribute(\"href\")\n",
    "        if not pdf_url:\n",
    "            continue\n",
    "        filename = os.path.join(output_dir, os.path.basename(pdf_url))\n",
    "\n",
    "        if os.path.exists(filename):\n",
    "            print(f\"Already downloaded: {filename}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Downloading: {pdf_url}\")\n",
    "        try:\n",
    "            r = requests.get(pdf_url)\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {pdf_url}: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "print(\"All PDFS found from number of pages provided are downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d158d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since delpher has the pdf files within each link, selenium collects and opens eachs instance on the search page and downloads the .pdf from there\n",
    "#Name url to scrape from, output directory and the number of pages to scrape\n",
    "output_dir = \"Nederlandschestaatscourantaaa\"\n",
    "max_pages = 78  # Adjust this to how many pages you want to scrape\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#Setup of Selenium\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "#Loop through all search result pages\n",
    "for page in range(1, max_pages + 1):\n",
    "    print(f\"Scraping page {page}...\")\n",
    "\n",
    "    url = f\"https://www.delpher.nl/nl/kranten/results?query=staatsblad&page={page}&maxperpage=50&cql%5B%5D=(date+_gte_+%2201-01-1946%22)&cql%5B%5D=(date+_lte_+%2231-12-2005%22)&cql%5B%5D=ppn+any+(400915472+OR+830850090)&coll=ddd\"\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        #Wait for results to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//a[contains(@href, '/nl/kranten/view?')]\"))\n",
    "        )\n",
    "        time.sleep(1)  # Optional fallback wait\n",
    "    except:\n",
    "        print(f\"No articles loaded on page {page}\")\n",
    "        continue\n",
    "\n",
    "    #Collect unique article URLs\n",
    "    article_link_elems = driver.find_elements(By.XPATH, \"//a[contains(@href, '/nl/kranten/view?')]\")\n",
    "    article_urls = list({elem.get_attribute(\"href\") for elem in article_link_elems if elem.get_attribute(\"href\")})\n",
    "\n",
    "    print(f\"Found {len(article_urls)} articles\")\n",
    "\n",
    "    #Visit & download each article from the urls\n",
    "    for article_url in article_urls:\n",
    "        print(f\"Opening article: {article_url}\")\n",
    "        driver.get(article_url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        try:\n",
    "            #Wait for the PDF download link\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//a[contains(@href, 'type=pdf') and contains(@href, '/api/resource')]\"))\n",
    "            )\n",
    "\n",
    "            pdf_link_elem = driver.find_element(\n",
    "                By.XPATH,\n",
    "                \"//a[contains(@href, 'type=pdf') and contains(@href, '/api/resource')]\"\n",
    "            )\n",
    "            pdf_url = pdf_link_elem.get_attribute(\"href\")\n",
    "\n",
    "            if pdf_url:\n",
    "                identifier = pdf_url.split(\"identifier=\")[1].split(\"&\")[0].replace(\":\", \"_\")\n",
    "                filename = os.path.join(output_dir, f\"{identifier}.pdf\")\n",
    "\n",
    "                if os.path.exists(filename):\n",
    "                    print(f\"Already downloaded: {filename}\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"Downloading PDF: {pdf_url}\")\n",
    "                r = requests.get(pdf_url)\n",
    "                r.raise_for_status()\n",
    "                with open(filename, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                print(f\"Saved: {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download from {article_url}: {e}\")\n",
    "\n",
    "# Close browser\n",
    "driver.quit()\n",
    "print(\"Done downloading all PDFs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
